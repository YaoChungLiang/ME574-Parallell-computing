{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 4 Evaluating Functions on Multi-Dimensional Grids\n",
    "\n",
    "In the notebook for Chapter 2 \"Function Evaluations and the Map Pattern\" we created\n",
    "serial and parallel implementations of the _map_ app to evaluate\n",
    "a trigonometric function on a one-dimensional grid of input data.\n",
    "The notebook for Chapter 3 \"Tools for Timing, Profiling, and Debugging\" is in development and will come out later). In this notebook, we create the _map2D_ and _map3D_ apps\n",
    "that extend the map pattern to perform function evaluation on two-dimensional (2D)\n",
    "and three-dimensional (3D) grids of input data.\n",
    "\n",
    "> CUDA natively supports indexing up\n",
    "to three dimensions so we will typically not\n",
    "continue on to higher dimensional\n",
    "grids. If you need to handle a problem which would naturally involve a geometric grid of dimension $D_g>3$, you can include a loop over additional dimensions within the kernel or you can map the index values for the $D_g$-dimensional geometric grid to the index values of a $D_c$-dimensional computational grid with $D_c \\leq 3$.\n",
    "\n",
    "\n",
    "# 4.1 Evaluating a Two-Dimensional Array of Function Values: The _map2D_ App\n",
    "\n",
    "We start by using the _map_ app as a template to expand upon and create\n",
    "the _map2D_ app that computes values of a function on a 2D grid of input points. For the sample code, we choose a new function of 2 variables:\n",
    "\n",
    "$$f_{2D}(x,y) = \\frac{sin(\\pi x) sinh(\\pi y)}{sinh(\\pi)}$$ (4.1)\n",
    "\n",
    "which involves both trigonometric and hyperbolic functions.\n",
    "\n",
    "As alluded to in the comment above, there are really 2 grids to deal with: a geometric grid of points $\\{x_p,y_q\\}=\\{x_0+p \\Delta x, y_0 + q \\Delta y \\}$ where we want to evaluate the function $f(x_p,y_q)$ and a computational grid of threads with indices `i,j`. \n",
    "\n",
    "While there is considerable freedom in choosing the relation between the geometric and computational grids, here we present a simple, intuitive, and very commonly used choice. We establish a 1:1 correspondence between the geometric and computational grids, by identifying $p$ with `i` and $q$ with `j` so that each 2-tuple of index values specifies a corresponding point on the geometric grid. \n",
    "\n",
    "Having established the relationship of the computational grid indices to the coordinate values for points on the geometric grid, we are now ready to look at implementations that map the function of 2 variables $f(x,y)$ on the 2D geometric grid. \n",
    "\n",
    "# 4.1 Serial Implementation\n",
    "\n",
    "Just like the one-dimensional _map_ app, our serial implementation of _map2D_ consists of 2 files:\n",
    "\n",
    "1. _apps/map2D/main.py_ which contains the `main()` function that initializes a 2D array, calls a function to evaluate the function on the 2D geometric grid,\n",
    "and plots the computed 2D array of function values.\n",
    "\n",
    "2. _apps/map2D/serial.py_ which computes the function values, stores them in the appropriate positions in the array, and returns the array of computed function values.\n",
    "\n",
    "Listing 4.1 and 4.3 together form the implementation of _map2D_. In both cases, there are only a few changes needed to transform the previously created _map_ app into the _map2D_ app.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NX, NY = 128, 64\n",
    "\n",
    "def main():\n",
    "\tx = np.linspace(0, 1, NX)\n",
    "\ty = np.linspace(0, 1, NY)\n",
    "\n",
    "\tfrom serial import fArray2D\n",
    "\tf = fArray2D(x, y)\n",
    "\n",
    "\tX, Y = np.meshgrid(x, y)\n",
    "\tplt.contourf(X, Y, f.T) #`T` is shorthand for `transpose()`\n",
    "\tplt.xlabel(\"X\")\n",
    "\tplt.ylabel(\"Y\")\n",
    "\tplt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n",
    "```\n",
    "\n",
    " $$ \\text{ Listing 4.1: }apps/map2D/main.py$$\n",
    " \n",
    "> For applications like creating multidimensional plots, the numpy function `meshgrid()` is often used to create tuples of coordinate values corresponding to points on a cartesian grid from 1D arrays of coordinate values along each coordinate direction. Here `x` and `y` are the arrays of 1D coordinate values, so  `np.meshgrid(x,y)` constructs the 2-tuples of $\\{x,y\\}$ values on the 2D geometric grid. `X,Y = np.meshgrid(x,y)` assigns the components of the tuple to 1D arrays `X` and `Y` to produce 1D arrays of coordinates of points that arise while traversing the grid (which, together with the 2D array of function values, correspond to the positional arguments expected for standard 2D plotting functions like `contourf()`.)\n",
    "\n",
    "Most of the differences between _apps/map/main.py_ and  _apps/map2D/main.py_ arise in defining additional variables for the added dimension. Listing 4.2 provides a comparison of the files with code from _apps/map/main/py_ appearing as comments after `#`. `#SAME` indicates that the same line appears in both files.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NX, NY = 128, 64 # N = 128\n",
    "\n",
    "def main():\n",
    "\tx = np.linspace(0, 1, NX) #SAME\n",
    "\ty = np.linspace(0, 1, NY)\n",
    "\n",
    "\tfrom serial import fArray2D #SAME\n",
    "\tf = fArray2D(x, y) #f = sArray(x)\n",
    "\n",
    "\tX, Y = np.meshgrid(x, y) #plot.plot(x,f,'bo')\n",
    "\tplt.contourf(X, Y, f.T) #`T` is shorthand for `transpose()`\n",
    "\tplt.xlabel(\"X\")\n",
    "\tplt.ylabel(\"Y\")\n",
    "\tplt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n",
    "```\n",
    "\n",
    "$$ \\text{Listing 4.2: Comparison of } apps/map2D/main.py \\text{ and } apps/map/main.py $$\n",
    "\n",
    "Instead of a single value, `N` , to define the array size, we use two separate values, `NX` and `NY` , to specify the number of points in each grid direction.\n",
    "`np.linspace()` is called a second time to create an array `y` of coordinate values along the $y$-axis. The name of the imported function is changed to `fArray2D()`. Lastly, The graphical output is a contour plot so the commands\n",
    "to create the plot requires the additional line calling `np.meshgrid()` to create the grid of $x,y$ coordinate pairs, and `plt.plot()` is replaced with `plt.contourf(X, Y, f.T)`.\n",
    "\n",
    "> The output array is transposed using `f.T` as shortthand for f.transpose()` to provide data in the format required by `contourf()`.\n",
    "\n",
    "The file _apps/map2D/serial.py_ defining the imported function is changed in much the same way to produce _apps/\n",
    "map2D/main.py_ as shown in Listing 4.3\n",
    "\n",
    "```\n",
    "1 import math\n",
    "2 import numpy as np\n",
    "3 PI = np.pi\n",
    "4 def f2D(x, y):\n",
    "5   return math.sin( PI *x)* math.sinh( PI *y)/ math.sinh( PI )\n",
    "6\n",
    "7 def fArray2D(x, y):\n",
    "8   nx = x.size\n",
    "9   ny = y.size\n",
    "10  f = np.empty((nx ,ny), dtype = np.float32)\n",
    "11\n",
    "12  for i in range(nx):\n",
    "13      for j in range(ny):\n",
    "14          f[i,j] = f2D(x[i], y[j])\n",
    "15  return f\n",
    "```\n",
    "$$\\text{Listing 4.3: } apps/map2D/serial.py$$\n",
    "\n",
    "All of the alterations are to accommodate the new dimension. On Lines 4-5, the function to be evaluated at each point is named `f2D()` and corresponds to Equation 4.1, $f_{2D}(x, y)$. Lines 8-9 assign to the variables `nx` and `ny` the sizes of the input coordinate arrays `x` and `y`. On line 10, an empty numpy array `f` of size `nx` $\\times$ `ny` is created to provide storage for the array of 32-bit function values to be computed. Lines 12-13 specify nested `for` loops that iterate over the range of index values corresponding to the size of the input arrays. On line 14, the function `f2D()` is evaluated with arguments corresponding to the coordinates of the corresponding point on the geometric grid (stored in `x[i],y[j]`), and the function value is stored as the corresponding array element `f[i,j]`.\n",
    "\n",
    "That concludes the serial implementation, and we are ready to parallelize.\n",
    "\n",
    "### 4.1.2 Parallel Implementation\n",
    "\n",
    "As with the 1D case, the plan for parallelization is straightforward. Modify the imported function `fArray2D()` so that the `for` loops in the serial implementation are replaced with the launch of a kernel function `fKernel2D[]()` that performs the computation previously contained in the loops.\n",
    "\n",
    "We store the modified code in _apps/map2D/parallel.py_ which is shown in Listing 4.4. The changes performed to get from _apps/map2D/serial.py_ to  _apps/map2D/parallel.py_ are described below, and you should compare listing 4.4 with Listing 4.3 as you read that description.\n",
    "\n",
    "```\n",
    "1 import math\n",
    "2 import numpy as np\n",
    "3 from numba import cuda\n",
    "4 PI = np.pi\n",
    "5 TPBX = 16\n",
    "6 TPBY = 16\n",
    "7\n",
    "8 @cuda.jit( device = True )\n",
    "9 def f2D(x, y):\n",
    "10  return math.sin( PI *x)* math.sinh( PI *y)/ math.sinh( PI )\n",
    "11\n",
    "12 @cuda.jit('void (f4[:] , f4[:] , f4[: ,:])')\n",
    "13 def fKernel2D(d_x , d_y , d_f):\n",
    "14  i , j = cuda.grid(2)\n",
    "15  nx , ny = d_f.shape\n",
    "16  if i < nx and j < ny:\n",
    "17      d_f[i,j] = f2D(d_x[i], d_y[j])\n",
    "18\n",
    "19 def fArray2D(x, y):\n",
    "20  nx = x.size\n",
    "21  ny = y.size\n",
    "22\n",
    "23  d_x = cuda.to_device(x)\n",
    "24  d_y = cuda.to_device(y)\n",
    "25  d_f = cuda.device_array((nx , ny), dtype = np.float32)\n",
    "26\n",
    "27  gridDims = ((nx + TPBX - 1)//TPBX ,\n",
    "28              (ny + TPBY - 1)// TPBY )\n",
    "29  blockDims = (TPBX , TPBY )\n",
    "30\n",
    "31  fKernel2D[ gridDims , blockDims ](d_x , d_y , d_f)\n",
    "32\n",
    "33  return d_f.copy_to_host()\n",
    "```\n",
    "\n",
    "$$ \\text{Listing 4.4: } apps/map2D/parallel.py$$\n",
    "\n",
    "Like the serial version, the file begins by importing relevant packages, now including numba's `cuda` package to provide support for parallelism. Lines 4-5 assign values for the variables, `TPBX` and `TPBY`, that are used to define the size (number of threads along each index direction) of the blocks to be established when the kernel is launched.\n",
    "\n",
    "> __Execution parameter limitations:__ In the implementation of CUDA, a number of design decisions had to be made that lead to practical limitations on how it can be used. We have mentioned one already: Grids of dimension higher than 3 are not supported. Here we run into another important limit: There is a maximum number of threads that are allowed in a block, and the limit is 1024 for almost all currently available GPUs. Since $64 \\times 64 = 4096$, a $64 \\times 64$ block would violate the restriction and generate an error, so the sample code launches a grid with $16 \\times 16$ blocks (each with 256 threads). See the CUDA documentation (or Wikipedia's CUDA page) for the full set of technical specifications and limitations.\n",
    "\n",
    "Lines 9-10 define the function `f2d` that is to be evaluated at each grid point. As in the 1D case, this function will be called from the kernel, so the decorator `@cuda.jit( device = True )` is included on line 8, immediately before the function definition to identify it as a device function.\n",
    "\n",
    "The definition of the kernel function `fKernel2D()` appears on lines 12-17 including the decorator `@cuda.jit('void(f4[:] , f4[:] , f4[: ,:])')` that accomplishes several goals:\n",
    "\n",
    "1. The decorator identifies `fKernel2D` as a kernel function to be launched from the host and executed on the device.\n",
    "2. It satisfies the requirement that a kernel cannot return a value by specifying the return type `void`.\n",
    "3. The code in the parentheses follow `void` indicate that there will be 3 arguments, two 1D arrays (with a single colon in the square brackets) followed by a 2D array (with `[]:,:]`). `f4` indicates that 4 bytes of memory are allocate for each entry in the array. Thus, `f4` provides an abbreviated alternative to `np.float32`.\n",
    "\n",
    "The definition statement on line 13 incidates that `fKernel2D` takes 3 arguments: the device array `d_x` that should store a copy of the $x$-coordinates,  the device array `d_y` that should store a copy of the $y$-coordinates, and the device array `d_f` to store the computed function values. On line 14, `i,j = cuda.grid(2)`,  defines the 2-tuple of index values. \n",
    "\n",
    "> Note that `i,j = cuda.grid(2)` is numba's handy abbreviation for the equivalent code:\n",
    "<br>`i = threadIdx.x + blockDim.x * blockIdx.x`\n",
    "<br>`j = threadIdx.y + blockDim.y * blockIdx.y`\n",
    "<br>the first line of which defines `i` according to the  1D index formula, while the second line defines `j` simply by changing the \"suffix\" from `.x` to `.y`.\n",
    "\n",
    "Line 15 assigns to `nx,ny` the dimensions of the array `d_f` to be computed. Line 16 tests whether the indices `i` and `j` are within the index bounds of `d_f` and, if so, `d_f[i,j] = f2D(d_x[i], d_y[j])` evaluates the function at the corresponding point on the geometric grid and stores the result as the indexed entry in `d_f`.\n",
    "\n",
    "> __Bounds checking:__ If an array dimension is not an exact multiple of the corresponding block size, then the \"last\" block (with the largest `blockIdx` value) will include index values that lie beyond the extent of the array. To avoid reading or writing into unintended portions of memory, the kernel should routinely include a bounds check similar to the code on line 16. _Later we will consider whether this produces a performance penalty due to_ ___thread divergence___.\n",
    "\n",
    "The remainder of the file, lines 19-33, defines the wrapper function `fArray2D()`. Lines 20-21 assign to `nx` and `ny` the length of the input arrays of coordinate values `x` and `y`. Lines 23-24  call `cuda.to_device()` to create `d_x` and `d_y` (as \"mirror\" copies of the inputs `x` and `y`) that give the kernel access to copies of the input data. On line 25, `d_f` is created to provide a `nx` $\\times$ `ny` device array to store the function values. Lines 27-29 set the values for the execution parameters. Here, both `gridDims` and `blockDims` are 2-tuples (for a 2D grid). The components of `blockDims` are assigned to match the specified values, `TPBX` and `TBPY`, and each dimension of `gridDims` is computed from the corresponding component of `blockDims` according to the same formula used in the 1D case. The call to execute the kernel launch appears on line 31: `fKernel2D[ gridDims , blockDims ](d_x , d_y , d_f)`. This call matches the format of the 1D version: kernel function name, followed by `[ gridDims , blockDims ]`, followed by parentheses containing the comma-separated list of arguments). Finally, on line 33, the array of computed vaues are copied back to the host by `return d_f.copy_to_host()` so the output is available for plotting on the CPU side.\n",
    "\n",
    "> __Device array methods:__ `d_f` was created as a device array object, so it comes with a variety of methods including `copy_to_host()` for copying data from device to host.\n",
    "\n",
    "> __Return value:__ Note carefully how the results get back to the host. A device array is defined in the wrapper function and included as a kernel argument. The kernel stores the output in the device array, but does ___NOT___ return anything to the host. It is the wrapper function, not the kernel itself, that returns the output to the host using `copy_to_host()`.\n",
    "\n",
    "That completes the description of _apps/map2D/parallel.py_, and all that remains is a minor change to _apps/map2D/main.py_. To run the parallelized version of the _map2D_ app, we want to use the parallel version of `fArray2D()` instead of the serial version used previously, so on line 12 of  _apps/map2D/main.py_, `from serial import fArray2D` should be replaced by `from parallel import fArray2D`.\n",
    "\n",
    "> __File locations:__ For the `import` statement to work as desired, the files to be imported (_apps/map2D/serial.py_ and _apps/map2D/parallel.py_) should be located in the same directory as _apps/map2D/main.py_. To import from other locations, directory location information must be provided for files from which is imported.\n",
    "\n",
    "\n",
    "\n",
    "## 4.2 Evaluating a Three-Dimensional Array of Function Values: The map3D App\n",
    "\n",
    "Having seen the implementation of _map2D_, which basically involved adding a second coordinate direction to _map_, the extension to compute function values on a 3D grid should seem relatively straightforward; it involves similarly adding a third coordinate direction. Let’s start by choosing a function with three parameters:\n",
    "\n",
    "$$f_{3D}(x,y,z) = \\frac{sinh(\\pi y)}{sinh(\\pi)} sin(\\pi x)  cos(\\pi z) $$\n",
    "\n",
    "Listings 4.5, 4.6, and 4.7 show the code for _apps/map3D/main.py_, _apps/map3D/serial.py__, and _apps/map3D/parallel.py_. For now, we settle for printing results to the terminal; we will return later to consider visualization of 3d grids of data.\n",
    "\n",
    "```\n",
    "File: main.py\n",
    "01: import numpy as np\n",
    "02: import matplotlib.pyplot as plt\n",
    "03: \n",
    "04: NX, NY, NZ = 8, 8, 16\n",
    "05: \n",
    "06: def main():\n",
    "07: \tx = np.linspace(0, 1, NX)\n",
    "08: \ty = np.linspace(0, 1, NY)\n",
    "09: \tz = np.linspace(0, 1, NZ)\n",
    "10: \n",
    "11: \tfrom serial import fArray3D\n",
    "12: \tf = fArray3D(x, y, z)\n",
    "13: \tprint(f)\n",
    "14: \n",
    "15: if __name__ == '__main__':\n",
    "16: \tmain()\n",
    "\n",
    "```\n",
    "\n",
    "$$ \\text{Listing 4.5: } apps/map3D/main.py$$\n",
    "\n",
    "Note that _apps/map3D/main.py_ is a bit simpler (without the plotting code) and that number of blocks in each direction is reduced so that the total number of threads in each block, $8 \\times 8 \\times 8 = 512$, is under the 1024 limit.\n",
    "\n",
    "At this point, the code will hopefully seem pretty readable, so you should read through it and then experiment using it to see how it works.\n",
    "\n",
    "```\n",
    "File: serial.py\n",
    "01: import math\n",
    "02: import numpy as np\n",
    "03: PI = np.pi\n",
    "04: def f3D(x0, y0, z0):\n",
    "05: \treturn math.sin(PI * x0) * math.cos(PI * z0) * math.sinh(PI * y0) / math.sinh(PI)\n",
    "06: \n",
    "07: def fArray3D(x, y, z):\n",
    "08: \tnx = x.shape[0]\n",
    "09: \tny = y.shape[0]\n",
    "10: \tnz = z.shape[0]\n",
    "11: \tf = np.empty(shape=[nx,ny,nz], dtype = np.float32)\n",
    "12: \tfor i in range(nx):\n",
    "13: \t\tfor j in range(ny):\n",
    "14: \t\t\tfor k in range(nz):\n",
    "15: \t\t\t\tf[i,j,k] = f3D(x[i], y[j], z[k])\n",
    "16: \treturn f\n",
    "```\n",
    "$$ \\text{Listing 4.6: } apps/map3D/serial.py$$\n",
    "\n",
    "\n",
    "```\n",
    "File: parallel.py\n",
    "01: import math\n",
    "02: import numpy as np\n",
    "03: from numba import jit, cuda, float32\n",
    "04: \n",
    "05: TPBX, TPBY, TPBZ = 8, 8, 8\n",
    "06: \n",
    "07: @cuda.jit(device = True)\n",
    "08: def f3D(x0, y0, z0):\n",
    "09: \treturn math.sin(np.pi * x0) * math.cos(np.pi * z0) * math.sinh(np.pi * y0) / math.sinh(np.pi)\n",
    "10: \n",
    "11: @cuda.jit\n",
    "12: def fKernel3D(d_f, d_x, d_y, d_z):\n",
    "13: \ti,j,k = cuda.grid(3)\n",
    "14: \tnx,ny,nz = d_f.shape\t\n",
    "15: \tif i < nx and j < ny and k < nz:\n",
    "16: \t\td_f[i,j,k] = f3D(d_x[i], d_y[j], d_z[k])\n",
    "17: \n",
    "18: def fArray3D(x, y, z):\n",
    "19: \tnx = x.shape[0]\n",
    "20: \tny = y.shape[0]\n",
    "21: \tnz = z.shape[0]\n",
    "22: \td_x = cuda.to_device(x)\n",
    "23: \td_y = cuda.to_device(y)\n",
    "24: \td_z = cuda.to_device(z)\n",
    "25: \td_f = cuda.device_array(shape = [nx,ny,nz], dtype = np.float32)\n",
    "26: \tgridDims = (nx+TPBX-1)//TPBX, (ny+TPBY-1)//TPBY, (nz+TPBZ-1)//TPBZ\n",
    "27: \tblockDims = TPBX, TPBY, TPBZ\n",
    "28: \tfKernel3D[gridDims, blockDims](d_f, d_x, d_y, d_z)\n",
    "29: \n",
    "30: \treturn d_f.copy_to_host()\n",
    "```\n",
    "$$ \\text{Listing 4.7: } apps/map3D/parallel.py$$\n",
    "\n",
    "\n",
    "The changes toget from_apps/map2D/serial.py_ to  _apps/map3D/serial.py_ include renaming and updating the functions `f3D()` and `fArray3D()`. Each has three inputs for the $x$, $y$, and $z$ directions. `fArray3D()` includes a triply nested loop on Lines 12-15 with indices `i`, `j`, and `k` with bounds determined from the shapes of the input arrays `x` , `y` and `z`. Each output is stored as a 32-bit float in the three-dimensional array `f`.\n",
    "\n",
    "The modifications to _apps/map3D/parallel.py_ are similar. The suffix on the function names is changed from 2D to 3D and each takes 3 arguments. The indices in the kernel are assigned using `i, j, k = cuda.grid(3)`, and the bounds are determined from the shape of the 3D output device array `d_f`. Bounds are checked in all three directions, and the computed function value is stored in the appropriate\n",
    "location in `d_f`.\n",
    "\n",
    "The wrapper function uses the newly introduced `TPBZ` to set the `.z`-component size of each block. The input arrays are copied to device arrays on lines 22-24, and a three-dimensional output device array\n",
    "is created on Line 25. The execution parameters, which are now both tuples of length 3, are specified on Lines 26-27, and the kernel call occurs on line 28. The wrapper once again ends by copying the results\n",
    "from the output device array to the host on Line 30.\n",
    "\n",
    "This finishes our discussion on setting up multi-dimensional grids. After reading carefullly through this notebook, you should be ready to do Homework 3 and then move on to further notebooks.\n",
    "\n",
    "## Suggested Projects\n",
    "\n",
    "1. Experiment with removing the data type specifications in _apps/map2D/main.py_. Do the serial and parallel results agree exactly without explicit dtype specification. Try removing the data type specifications from the signature as well.\n",
    "\n",
    "2. Time the execution of the serial and parallel implementations of `fArray2D()`. Characterize the acceleration due to parallelization for a range of array sizes.\n",
    "\n",
    "3. What is largest square block size for which you can execute `fArray2D()`? What CUDA limit do you run into? What error message is generated?\n",
    "when the requested block is too large?\n",
    "\n",
    "4. Add a signature to the `fKernel3D()` function decorator.\n",
    "\n",
    "5. Time the execution of the serial and parallel implementations of `fArray3D()`. Characterize the acceleration due to parallelization for a range of array sizes.\n",
    "\n",
    "6. What is largest cubic block size for which you can execute `fArray3D()`? What CUDA limit do you run into? What error message is generated when the requested block is too large?\n",
    "\n",
    "7. Experiment with execution parameter specifications that change the \"aspect ratio\" of your blocks (i.e. square vs. rectangular blocks). Can you detect any patterns about how aspect ratio changes affect kernel execution times?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
